### Description 

This project performs a time series analysis and prediction of stock price using LSTM.

## Part 1: Data loading & exploration
### 1.1 Preprocessing (normalization)

The data contains a history of 150 days of APPL. First, preprocess the data for use with our model


```python
# download the data
!wget https://raw.githubusercontent.com/tonylaioffer/stock-prediction-lstm-using-keras/master/data/sandp500/all_stocks_5yr.csv
```

    --2020-09-20 12:47:37--  https://raw.githubusercontent.com/tonylaioffer/stock-prediction-lstm-using-keras/master/data/sandp500/all_stocks_5yr.csv
    Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...
    Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 29580549 (28M) [text/plain]
    Saving to: ‘all_stocks_5yr.csv’
    
    all_stocks_5yr.csv  100%[===================>]  28.21M  74.6MB/s    in 0.4s    
    
    2020-09-20 12:47:38 (74.6 MB/s) - ‘all_stocks_5yr.csv’ saved [29580549/29580549]
    



```python
!ls .
```

    all_stocks_5yr.csv  sample_data



```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
!pip install torch
import torch.nn as nn
import torch
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
```

    Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)
    Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)
    Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)



```python
# Importing the training set
dataset = data = pd.read_csv('./all_stocks_5yr.csv')
dataset_cl = dataset[dataset['Name']=='SWKS'].close.values

# dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')
# training_set = dataset_train.iloc[:, 1:2].values
```


```python
# Feature Scaling
from sklearn.preprocessing import MinMaxScaler

sc = MinMaxScaler(feature_range = (0, 1))

# scale the data
dataset_cl = dataset_cl.reshape(dataset_cl.shape[0], 1)
dataset_cl = sc.fit_transform(dataset_cl)
```

## Part 2:  Time series segmentation

We can treat the time series prediction problem as a regression problem, hence we need to cut the series into pieces in order to apply our model


```python
#Create a function to process the data into 7 day look back slices
def processData(data, lb):
    X, Y = [], []
    for i in range(len(data) - lb - 1):
        X.append(data[i: (i + lb), 0])
        Y.append(data[(i + lb), 0])
    return np.array(X), np.array(Y)
X, y = processData(dataset_cl, 7)

```

## Part 3: Split training and testing sets




```python
X_train, X_test = X[:int(X.shape[0]*0.80)],X[int(X.shape[0]*0.80):]
y_train, y_test = y[:int(y.shape[0]*0.80)],y[int(y.shape[0]*0.80):]
print(X_train.shape[0])
print(X_test.shape[0])
print(y_train.shape[0])
print(y_test.shape[0])

# reshaping
X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))
```

    1000
    251
    1000
    251


## Part 4:  Build and run an RNN regression model

Using Torch to set up a two hidden layer RNN:
  - layer 1 uses and LSTM module with 5 hidden units
  - layer 2 uses a fully connected module with one unit
  - use MSE as our loss function


```python
class RNN(nn.Module):
    def __init__(self, i_size, h_size, n_layers, o_size, dropout=0.1, bidirectional=True):
        super(RNN, self).__init__()
        self.num_directions = bidirectional+1
        self.rnn = nn.LSTM(
            input_size=i_size,
            hidden_size=h_size,
            num_layers=n_layers,
            dropout=dropout,
            bidirectional=bidirectional
        )
        self.out = nn.Linear(h_size, o_size)

    def forward(self, x, h_state):
        r_out, hidden_state = self.rnn(x, h_state)
        
        hidden_size = hidden_state[-1].size(-1)
        r_out = r_out.view(-1, self.num_directions, hidden_size)
        outs = self.out(r_out)
        
        return outs, hidden_state
```


```python
# Globals

INPUT_SIZE = 7
HIDDEN_SIZE = 64
NUM_LAYERS = 3
OUTPUT_SIZE = 1
BATCH_SIZE = 128
# Hyper parameters

learning_rate = 0.001
num_epochs = 300

rnn = RNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE,bidirectional=False)
rnn.cuda()

optimiser = torch.optim.Adam(rnn.parameters(), lr=learning_rate)
criterion = nn.MSELoss()

hidden_state = None
```


```python
rnn
```




    RNN(
      (rnn): LSTM(7, 64, num_layers=3, dropout=0.1)
      (out): Linear(in_features=64, out_features=1, bias=True)
    )




```python
history = []
X_test_cuda = torch.tensor(X_test).float().cuda()
y_test_cuda = torch.tensor(y_test).float().cuda()

# we use all the data in one batch
inputs_cuda = torch.tensor(X_train).float().cuda()
labels_cuda = torch.tensor(y_train).float().cuda()

for epoch in range(num_epochs):
    rnn.train()
    output, _ = rnn(inputs_cuda, hidden_state) 
    # print(output.size())

    loss = criterion(output[:,0,:].view(-1), labels_cuda)
    optimiser.zero_grad()
    loss.backward()                     # back propagation
    optimiser.step()                    # update the parameters
    
    if epoch % 20 == 0:
        rnn.eval()
        test_output, _ = rnn(X_test_cuda, hidden_state)
        test_loss = criterion(test_output.view(-1), y_test_cuda)
        print('epoch {}, loss {}, eval loss {}'.format(epoch, loss.item(), test_loss.item()))
    else:
        print('epoch {}, loss {}'.format(epoch, loss.item()))
    history.append(loss.item())
```

    epoch 0, loss 0.16063907742500305, eval loss 0.5001010894775391
    epoch 1, loss 0.14192374050617218
    epoch 2, loss 0.12446314096450806
    epoch 3, loss 0.10815636068582535
    epoch 4, loss 0.09325005859136581
    epoch 5, loss 0.07984400540590286
    epoch 6, loss 0.06824441254138947
    epoch 7, loss 0.0598384365439415
    epoch 8, loss 0.05760418251156807
    epoch 9, loss 0.060530681163072586
    epoch 10, loss 0.06469206511974335
    epoch 11, loss 0.06403744965791702
    epoch 12, loss 0.058708157390356064
    epoch 13, loss 0.052014514803886414
    epoch 14, loss 0.04666827991604805
    epoch 15, loss 0.042635682970285416
    epoch 16, loss 0.04017156735062599
    epoch 17, loss 0.03719509765505791
    epoch 18, loss 0.03383466228842735
    epoch 19, loss 0.02960609644651413
    epoch 20, loss 0.024525437504053116, eval loss 0.023796983063220978
    epoch 21, loss 0.019092882052063942
    epoch 22, loss 0.015922268852591515
    epoch 23, loss 0.016516676172614098
    epoch 24, loss 0.015086897648870945
    epoch 25, loss 0.010934242978692055
    epoch 26, loss 0.008091868832707405
    epoch 27, loss 0.008382797241210938
    epoch 28, loss 0.009119914844632149
    epoch 29, loss 0.008461975492537022
    epoch 30, loss 0.006429332308471203
    epoch 31, loss 0.0053951554000377655
    epoch 32, loss 0.006455251947045326
    epoch 33, loss 0.00661782780662179
    epoch 34, loss 0.0052537862211465836
    epoch 35, loss 0.004223579540848732
    epoch 36, loss 0.004213549196720123
    epoch 37, loss 0.004731409251689911
    epoch 38, loss 0.004578358493745327
    epoch 39, loss 0.0040243458934128284
    epoch 40, loss 0.003104561474174261, eval loss 0.008516821078956127
    epoch 41, loss 0.0031274815555661917
    epoch 42, loss 0.0034297010861337185
    epoch 43, loss 0.0032677967101335526
    epoch 44, loss 0.0029452492017298937
    epoch 45, loss 0.0028890680987387896
    epoch 46, loss 0.0029003210365772247
    epoch 47, loss 0.00280022039078176
    epoch 48, loss 0.0029555889777839184
    epoch 49, loss 0.0029840196948498487
    epoch 50, loss 0.0027210249099880457
    epoch 51, loss 0.003019520314410329
    epoch 52, loss 0.0026350102853029966
    epoch 53, loss 0.002908828668296337
    epoch 54, loss 0.002835012972354889
    epoch 55, loss 0.0027825310826301575
    epoch 56, loss 0.002583061810582876
    epoch 57, loss 0.0024350634776055813
    epoch 58, loss 0.002464243909344077
    epoch 59, loss 0.0025383993051946163
    epoch 60, loss 0.0026132490020245314, eval loss 0.013729602098464966
    epoch 61, loss 0.002441540826112032
    epoch 62, loss 0.002236725063994527
    epoch 63, loss 0.0023696166463196278
    epoch 64, loss 0.002170815132558346
    epoch 65, loss 0.0022582069505006075
    epoch 66, loss 0.0023402555380016565
    epoch 67, loss 0.0020481424871832132
    epoch 68, loss 0.002201133407652378
    epoch 69, loss 0.002213094849139452
    epoch 70, loss 0.0022869131062179804
    epoch 71, loss 0.0020597863476723433
    epoch 72, loss 0.0019802835304290056
    epoch 73, loss 0.0019707316532731056
    epoch 74, loss 0.0019326366018503904
    epoch 75, loss 0.002089007059112191
    epoch 76, loss 0.002122096484526992
    epoch 77, loss 0.0021014653611928225
    epoch 78, loss 0.002026613336056471
    epoch 79, loss 0.001705311005935073
    epoch 80, loss 0.0020329877734184265, eval loss 0.00785771943628788
    epoch 81, loss 0.002016425132751465
    epoch 82, loss 0.001993852900341153
    epoch 83, loss 0.001943099545314908
    epoch 84, loss 0.001865967526100576
    epoch 85, loss 0.0019261506386101246
    epoch 86, loss 0.0018684807000681758
    epoch 87, loss 0.001892341417260468
    epoch 88, loss 0.0017533054342493415
    epoch 89, loss 0.0019031490664929152
    epoch 90, loss 0.0018102232133969665
    epoch 91, loss 0.001718480372801423
    epoch 92, loss 0.0018761728424578905
    epoch 93, loss 0.0019384222105145454
    epoch 94, loss 0.0018593211425468326
    epoch 95, loss 0.0019305291352793574
    epoch 96, loss 0.0017602101434022188
    epoch 97, loss 0.001837855321355164
    epoch 98, loss 0.0017324708169326186
    epoch 99, loss 0.00180678884498775
    epoch 100, loss 0.0017647071508690715, eval loss 0.007144935894757509
    epoch 101, loss 0.0018267809646204114
    epoch 102, loss 0.001710367389023304
    epoch 103, loss 0.0019220520043745637
    epoch 104, loss 0.0017912894254550338
    epoch 105, loss 0.0017950646579265594
    epoch 106, loss 0.0017770436825230718
    epoch 107, loss 0.0017632447415962815
    epoch 108, loss 0.0018747415160760283
    epoch 109, loss 0.001743743778206408
    epoch 110, loss 0.0016620929818600416
    epoch 111, loss 0.0017287873197346926
    epoch 112, loss 0.0016843464691191912
    epoch 113, loss 0.0016403718618676066
    epoch 114, loss 0.0017040956299751997
    epoch 115, loss 0.0016407952643930912
    epoch 116, loss 0.0017227726057171822
    epoch 117, loss 0.0015981815522536635
    epoch 118, loss 0.0017525452421978116
    epoch 119, loss 0.0017593472730368376
    epoch 120, loss 0.0016407854855060577, eval loss 0.006524365860968828
    epoch 121, loss 0.001687219482846558
    epoch 122, loss 0.0018203514628112316
    epoch 123, loss 0.0016420393949374557
    epoch 124, loss 0.0015819782856851816
    epoch 125, loss 0.0017822670051828027
    epoch 126, loss 0.00175719044636935
    epoch 127, loss 0.001741276471875608
    epoch 128, loss 0.0016384421614930034
    epoch 129, loss 0.0016859958413988352
    epoch 130, loss 0.0016875979490578175
    epoch 131, loss 0.0016421113396063447
    epoch 132, loss 0.0016497649485245347
    epoch 133, loss 0.0016180038219317794
    epoch 134, loss 0.0016719687264412642
    epoch 135, loss 0.0016952166333794594
    epoch 136, loss 0.0015906797489151359
    epoch 137, loss 0.001612957799807191
    epoch 138, loss 0.001586953760124743
    epoch 139, loss 0.0016995200421661139
    epoch 140, loss 0.0016731893410906196, eval loss 0.006259959191083908
    epoch 141, loss 0.0016367379575967789
    epoch 142, loss 0.0015774598577991128
    epoch 143, loss 0.0016512315487489104
    epoch 144, loss 0.0016752826049923897
    epoch 145, loss 0.0015988588565960526
    epoch 146, loss 0.0014581502182409167
    epoch 147, loss 0.001651356928050518
    epoch 148, loss 0.001638715504668653
    epoch 149, loss 0.0016901902854442596
    epoch 150, loss 0.0015017141122370958
    epoch 151, loss 0.0016089902492240071
    epoch 152, loss 0.001653408631682396
    epoch 153, loss 0.0016384333139285445
    epoch 154, loss 0.0015241769142448902
    epoch 155, loss 0.0015626857057213783
    epoch 156, loss 0.001625591074116528
    epoch 157, loss 0.0016077287727966905
    epoch 158, loss 0.001569705898873508
    epoch 159, loss 0.0014447654830291867
    epoch 160, loss 0.0015970371896401048, eval loss 0.005856891628354788
    epoch 161, loss 0.0017953601200133562
    epoch 162, loss 0.0014356467872858047
    epoch 163, loss 0.001561684999614954
    epoch 164, loss 0.0014762665377929807
    epoch 165, loss 0.0015487322816625237
    epoch 166, loss 0.0015525626949965954
    epoch 167, loss 0.0016901867929846048
    epoch 168, loss 0.0015079017030075192
    epoch 169, loss 0.0017160676652565598
    epoch 170, loss 0.001501990482211113
    epoch 171, loss 0.001624539727345109
    epoch 172, loss 0.0015682545490562916
    epoch 173, loss 0.0015520898159593344
    epoch 174, loss 0.001524816150777042
    epoch 175, loss 0.0016036027809605002
    epoch 176, loss 0.0015810885233804584
    epoch 177, loss 0.001588350860401988
    epoch 178, loss 0.001480327919125557
    epoch 179, loss 0.001567700644955039
    epoch 180, loss 0.001559595693834126, eval loss 0.005550919566303492
    epoch 181, loss 0.0015321477549150586
    epoch 182, loss 0.001354454318061471
    epoch 183, loss 0.001596848014742136
    epoch 184, loss 0.0014748163521289825
    epoch 185, loss 0.0015000689309090376
    epoch 186, loss 0.0015044130850583315
    epoch 187, loss 0.0014549861662089825
    epoch 188, loss 0.0015060389414429665
    epoch 189, loss 0.0016001957701519132
    epoch 190, loss 0.0014527139719575644
    epoch 191, loss 0.0016248194733634591
    epoch 192, loss 0.001379824010655284
    epoch 193, loss 0.0015233593294396996
    epoch 194, loss 0.001570831867866218
    epoch 195, loss 0.0014609023928642273
    epoch 196, loss 0.00145455461461097
    epoch 197, loss 0.0013944024685770273
    epoch 198, loss 0.001449647475965321
    epoch 199, loss 0.0014058242086321115
    epoch 200, loss 0.0015284533146768808, eval loss 0.005247343797236681
    epoch 201, loss 0.0013405709760263562
    epoch 202, loss 0.0014997294638305902
    epoch 203, loss 0.001502719707787037
    epoch 204, loss 0.0015488065546378493
    epoch 205, loss 0.0014224676415324211
    epoch 206, loss 0.0014731634873896837
    epoch 207, loss 0.0014472664333879948
    epoch 208, loss 0.0014134392840787768
    epoch 209, loss 0.0015238336054608226
    epoch 210, loss 0.0015049281064420938
    epoch 211, loss 0.0016723360167816281
    epoch 212, loss 0.0014527473831549287
    epoch 213, loss 0.0013404606143012643
    epoch 214, loss 0.0015711935702711344
    epoch 215, loss 0.001546837156638503
    epoch 216, loss 0.0014111176133155823
    epoch 217, loss 0.0014664905611425638
    epoch 218, loss 0.0013884042855352163
    epoch 219, loss 0.0015541689936071634
    epoch 220, loss 0.0013751123333349824, eval loss 0.005029600113630295
    epoch 221, loss 0.0015080533921718597
    epoch 222, loss 0.001495101023465395
    epoch 223, loss 0.0014903564006090164
    epoch 224, loss 0.001396252540871501
    epoch 225, loss 0.0013252263888716698
    epoch 226, loss 0.0014293482527136803
    epoch 227, loss 0.001409374293871224
    epoch 228, loss 0.0014370442368090153
    epoch 229, loss 0.001391346799209714
    epoch 230, loss 0.0015849993797019124
    epoch 231, loss 0.0016075465828180313
    epoch 232, loss 0.0013700848212465644
    epoch 233, loss 0.001408495008945465
    epoch 234, loss 0.0013650195905938745
    epoch 235, loss 0.0014632801758125424
    epoch 236, loss 0.0012785234721377492
    epoch 237, loss 0.001403552363626659
    epoch 238, loss 0.0014157624682411551
    epoch 239, loss 0.0014276390429586172
    epoch 240, loss 0.0013586829882115126, eval loss 0.004918730352073908
    epoch 241, loss 0.001491639413870871
    epoch 242, loss 0.0014145812019705772
    epoch 243, loss 0.0014291395200416446
    epoch 244, loss 0.0013939698692411184
    epoch 245, loss 0.0012895300751551986
    epoch 246, loss 0.0012981313047930598
    epoch 247, loss 0.00142889772541821
    epoch 248, loss 0.0013718202244490385
    epoch 249, loss 0.00132059957832098
    epoch 250, loss 0.0014178333804011345
    epoch 251, loss 0.0012693048920482397
    epoch 252, loss 0.0013608485460281372
    epoch 253, loss 0.0013684568693861365
    epoch 254, loss 0.0014995590317994356
    epoch 255, loss 0.0012524890480563045
    epoch 256, loss 0.0014188466593623161
    epoch 257, loss 0.0013285306049510837
    epoch 258, loss 0.0012932424433529377
    epoch 259, loss 0.0015088783111423254
    epoch 260, loss 0.001237632823176682, eval loss 0.004814848303794861
    epoch 261, loss 0.0013741861330345273
    epoch 262, loss 0.0013520675711333752
    epoch 263, loss 0.0013709071790799499
    epoch 264, loss 0.0013507810654118657
    epoch 265, loss 0.0013732679653912783
    epoch 266, loss 0.0015323408879339695
    epoch 267, loss 0.0012440462596714497
    epoch 268, loss 0.001278088428080082
    epoch 269, loss 0.0013166971039026976
    epoch 270, loss 0.001264536869712174
    epoch 271, loss 0.0014109337935224175
    epoch 272, loss 0.0013125846162438393
    epoch 273, loss 0.0013321198057383299
    epoch 274, loss 0.0013591054594144225
    epoch 275, loss 0.0013398363953456283
    epoch 276, loss 0.001406948664225638
    epoch 277, loss 0.001393385580740869
    epoch 278, loss 0.001375768450088799
    epoch 279, loss 0.0013081221841275692
    epoch 280, loss 0.0013223921414464712, eval loss 0.004740155301988125
    epoch 281, loss 0.0014553299406543374
    epoch 282, loss 0.0014827308477833867
    epoch 283, loss 0.0014462036779150367
    epoch 284, loss 0.0013839204329997301
    epoch 285, loss 0.0013154542539268732
    epoch 286, loss 0.0013250134652480483
    epoch 287, loss 0.0013215026119723916
    epoch 288, loss 0.001319398288615048
    epoch 289, loss 0.0013100645737722516
    epoch 290, loss 0.0012894388055428863
    epoch 291, loss 0.0013484260998666286
    epoch 292, loss 0.0013206639559939504
    epoch 293, loss 0.0011777770705521107
    epoch 294, loss 0.0012788678286597133
    epoch 295, loss 0.001323773874901235
    epoch 296, loss 0.0013919426128268242
    epoch 297, loss 0.001324010780081153
    epoch 298, loss 0.0013195036444813013
    epoch 299, loss 0.0013718546833842993



```python
rnn.out.weight 
```




    Parameter containing:
    tensor([[-0.1136, -0.0049, -0.0863,  0.0839, -0.0431,  0.0720, -0.0754,  0.0587,
              0.0388, -0.0890,  0.0887,  0.0521,  0.0298, -0.0561, -0.0076,  0.0679,
             -0.0869, -0.1098,  0.1289,  0.1051, -0.0713,  0.0983, -0.0061, -0.0013,
              0.0399,  0.1140, -0.0728,  0.1209, -0.0054,  0.0936, -0.1109,  0.1325,
             -0.1052, -0.0177,  0.0295,  0.1134, -0.1144,  0.0241,  0.0193, -0.1014,
             -0.0460, -0.0415,  0.0890, -0.0245, -0.0257,  0.0975,  0.0064,  0.0185,
             -0.0383,  0.0981, -0.0201,  0.0518,  0.0217, -0.1181,  0.0744, -0.0941,
              0.0706, -0.0512, -0.0367, -0.1284, -0.0883,  0.0660, -0.0860, -0.0753]],
           device='cuda:0', requires_grad=True)




```python
for param in rnn.parameters():
    print(param.data)
```

    tensor([[-0.0826,  0.0138, -0.0111,  ...,  0.1121,  0.0800,  0.0183],
            [ 0.1293, -0.0042, -0.0208,  ..., -0.0743,  0.1239,  0.0805],
            [ 0.0003,  0.0441,  0.0333,  ..., -0.1145,  0.0070, -0.0206],
            ...,
            [-0.0935, -0.0190,  0.0421,  ..., -0.0841,  0.0660, -0.1582],
            [ 0.0212,  0.0652, -0.0722,  ..., -0.0202,  0.1602,  0.1626],
            [-0.0556, -0.0478, -0.0030,  ...,  0.0963,  0.1304,  0.0737]],
           device='cuda:0')
    tensor([[-0.0217,  0.0502,  0.0262,  ..., -0.0307,  0.0354,  0.0415],
            [-0.0054,  0.0026,  0.0055,  ..., -0.1507, -0.0599,  0.1254],
            [-0.0904, -0.0870,  0.0055,  ..., -0.0973, -0.0118, -0.0754],
            ...,
            [-0.0666, -0.1163,  0.0361,  ..., -0.1056, -0.0274,  0.0839],
            [ 0.0590,  0.0608, -0.0022,  ...,  0.1008,  0.0355, -0.0264],
            [ 0.0816, -0.0806, -0.0305,  ...,  0.0831,  0.1242, -0.0709]],
           device='cuda:0')
    tensor([ 0.1208,  0.0989,  0.0263,  0.0844,  0.0881,  0.1324,  0.0899,  0.1064,
            -0.0227, -0.0660, -0.1008,  0.0527, -0.0175,  0.0311, -0.0255,  0.0763,
            -0.0168,  0.1348, -0.0320,  0.1156,  0.0715, -0.0692, -0.0976, -0.0840,
             0.1133, -0.1032,  0.1152, -0.0539, -0.0429,  0.0020, -0.0629,  0.0506,
             0.0130,  0.0392, -0.1150,  0.0248,  0.0064,  0.0496,  0.0496,  0.0191,
             0.0014,  0.0536,  0.1261,  0.0291, -0.0303,  0.1424,  0.1055, -0.0069,
            -0.0435,  0.0511,  0.0628,  0.1009, -0.0901, -0.0817, -0.0661,  0.0431,
             0.0594,  0.1355,  0.0058, -0.0264,  0.1204, -0.0585, -0.0413, -0.0977,
             0.0112, -0.0203, -0.0994,  0.0986, -0.0606,  0.0973, -0.1105,  0.0407,
             0.0340,  0.1339, -0.0072, -0.0100, -0.0534,  0.0244,  0.1208,  0.1096,
             0.0220,  0.0272, -0.1359,  0.0290, -0.0313,  0.0229,  0.0532,  0.0393,
             0.0003,  0.0235, -0.0705,  0.0304,  0.0837, -0.0283, -0.0522,  0.0016,
            -0.1137, -0.0918,  0.0945,  0.0876,  0.1072, -0.0494,  0.0327, -0.1119,
             0.0768,  0.0516,  0.0016, -0.0427, -0.0513, -0.0714, -0.0899,  0.0258,
             0.0681, -0.0667,  0.0645,  0.0650, -0.1170,  0.0899,  0.0810,  0.0733,
             0.0590,  0.1391,  0.0356,  0.0511,  0.0122,  0.0958,  0.1024,  0.0868,
             0.0098, -0.0639,  0.0684, -0.0216, -0.0483,  0.1040,  0.0055, -0.0032,
            -0.0508, -0.1025, -0.0099,  0.0357,  0.1078,  0.1276,  0.0759, -0.0575,
             0.0899, -0.0490,  0.0283,  0.0829,  0.1157,  0.0955, -0.1064, -0.0349,
            -0.0480,  0.0983,  0.0948,  0.0462,  0.0233, -0.0739,  0.0685,  0.0141,
            -0.0134, -0.0292,  0.0525,  0.0001, -0.0681,  0.0780,  0.1034, -0.1241,
             0.0874, -0.0227,  0.0505, -0.0128,  0.0268, -0.0105,  0.0596, -0.0660,
             0.0805, -0.0796,  0.1243,  0.0388, -0.0428,  0.0452, -0.0328, -0.0307,
            -0.0220,  0.1193, -0.0392,  0.0795,  0.0085, -0.0772,  0.0993,  0.0834,
            -0.0527,  0.1006,  0.0141,  0.1336,  0.0846,  0.0982, -0.0235, -0.1036,
             0.0856, -0.0535, -0.0679,  0.1152,  0.0189,  0.1053, -0.0305, -0.0018,
             0.0995, -0.0251, -0.1151,  0.1211, -0.0011, -0.0137, -0.1187, -0.0824,
             0.0322,  0.1409, -0.0852, -0.0232,  0.0762,  0.0284,  0.0950, -0.0495,
             0.0953,  0.0997,  0.0782, -0.0493, -0.0296, -0.0106, -0.1231, -0.0111,
            -0.0749,  0.1202,  0.0887, -0.0286, -0.0414, -0.0834, -0.1007,  0.0879,
             0.0804,  0.0295, -0.0525, -0.0816,  0.0908,  0.0464,  0.0210, -0.0399,
            -0.0107, -0.0689, -0.0125,  0.0312, -0.0885,  0.0118,  0.0566, -0.1094],
           device='cuda:0')
    tensor([ 1.2364e-01,  1.1111e-01,  6.0040e-02, -2.8132e-02, -5.5346e-03,
            -6.4703e-02,  1.1145e-01,  1.0416e-01,  5.1305e-02,  4.0172e-02,
             4.2818e-03, -6.3730e-02, -6.9980e-02,  3.3376e-02,  2.0751e-02,
            -9.4535e-02,  4.0884e-02, -2.1505e-02, -1.0345e-01, -9.9577e-02,
            -7.1115e-02,  2.7830e-02,  7.2989e-02, -2.3091e-02, -8.3690e-05,
             8.4372e-02,  5.3424e-02,  1.3018e-01,  1.2887e-01, -2.9229e-02,
             5.8708e-02, -3.8990e-02, -3.6842e-03,  9.1111e-02,  7.2665e-02,
            -1.0470e-02,  1.7425e-03, -7.9577e-02,  3.7264e-04, -2.7786e-02,
            -8.0190e-02,  4.8129e-02,  8.0007e-02,  1.0983e-01, -4.6191e-02,
            -6.3293e-02, -8.4468e-02,  8.8228e-02,  8.5506e-02,  2.2903e-02,
             8.3692e-02, -4.1046e-02, -5.7161e-02, -1.9201e-02, -5.1817e-02,
             9.1314e-02,  2.8395e-02, -6.1100e-02, -9.8812e-02, -8.4790e-02,
             7.8864e-02,  4.4882e-03, -6.2646e-02, -8.5108e-02, -4.6410e-02,
             6.3615e-02,  5.5332e-02, -8.6273e-03,  8.8239e-02, -6.1223e-02,
             5.4696e-02,  1.2150e-02,  6.6081e-02,  1.7531e-02,  6.2024e-02,
             3.7303e-02,  1.2951e-01, -4.2846e-02,  9.8561e-02,  5.6335e-02,
            -1.3391e-02, -1.0734e-01, -7.1491e-02,  1.1641e-01, -9.3780e-02,
            -6.7452e-02, -1.3736e-01, -9.2038e-02, -6.6826e-02, -1.7261e-02,
            -1.0456e-01,  5.0513e-02, -4.8880e-02,  3.7243e-04,  1.2748e-01,
             1.4205e-03,  3.3737e-02, -3.2163e-02, -3.4272e-03, -4.6292e-02,
             5.0911e-02,  1.2583e-02,  2.7342e-02, -9.0205e-02, -3.5416e-02,
             6.9576e-02,  6.1012e-02,  1.3729e-01, -1.3474e-01, -5.3914e-02,
            -1.0628e-01, -1.5304e-02, -6.2095e-02,  1.0388e-01,  6.8927e-02,
             3.2014e-02, -7.1806e-03, -6.0917e-02,  8.8085e-02,  6.1719e-02,
            -8.3813e-02, -2.8904e-02, -3.1800e-02, -3.5203e-03, -4.4754e-02,
            -2.4709e-02, -4.8026e-02,  5.7492e-02, -2.2082e-02,  9.4722e-02,
             2.9582e-02,  4.4932e-02,  7.5062e-02,  1.1632e-01, -2.8767e-02,
             1.0880e-01, -3.9274e-02,  1.4589e-02,  8.5505e-02, -1.0254e-01,
            -6.8920e-03,  1.6142e-02, -2.8555e-03,  3.1334e-02,  2.1297e-02,
             1.0779e-01,  9.6952e-03, -3.6708e-04, -8.6087e-02, -9.2455e-02,
            -4.9464e-02,  5.5756e-02, -7.0443e-02,  1.0881e-01,  1.0931e-01,
             2.2958e-02,  1.1989e-02,  3.5542e-02,  6.6265e-02, -1.0099e-01,
            -7.2497e-03,  8.5901e-02,  1.5056e-03, -5.6651e-03,  1.2746e-02,
            -1.0056e-01,  7.7364e-02,  3.9158e-02,  2.6237e-02,  6.9341e-02,
            -9.8956e-02, -3.0432e-02, -1.2287e-01,  2.5378e-02,  1.0067e-01,
             6.0925e-02,  1.1549e-01,  8.5552e-02, -3.4282e-03,  1.0350e-01,
            -3.5216e-02, -6.1058e-02, -8.7521e-02, -9.4514e-02, -1.2418e-01,
            -3.0481e-02, -1.2023e-01,  1.0048e-01,  4.4558e-02, -4.2742e-02,
            -4.0284e-02, -3.1284e-02, -8.5052e-02,  4.8708e-02, -4.8097e-02,
             1.1141e-01,  1.1155e-01,  2.3921e-03, -1.8453e-02, -6.4789e-02,
            -6.7475e-02,  3.3435e-02,  6.9859e-02,  1.4006e-01,  1.6019e-01,
            -5.7949e-02,  8.5596e-02,  1.1260e-02, -1.6438e-02,  1.5358e-02,
            -2.2602e-02,  7.4835e-03,  9.8003e-02,  1.8919e-02, -1.8976e-02,
            -3.5634e-02, -1.0932e-01,  8.4280e-02,  1.0341e-01, -5.8816e-03,
            -1.0648e-01,  8.9389e-02,  1.3472e-01, -1.4794e-02, -1.8647e-02,
            -1.2460e-01,  3.1044e-02,  8.4522e-03,  8.1904e-02,  6.5866e-02,
             1.1101e-01, -1.1407e-01,  2.0286e-02,  1.2056e-01,  1.6109e-01,
             1.0464e-01, -2.0367e-02,  1.0419e-01, -4.0109e-02,  3.2832e-02,
             7.8865e-02,  9.3484e-02, -9.5146e-02, -5.9289e-02, -1.0306e-01,
             2.5438e-02,  9.8208e-02,  4.3817e-02, -1.2285e-02, -5.1142e-02,
             1.8246e-02,  4.2386e-02,  3.7014e-02,  6.6973e-02,  3.3309e-02,
             3.7227e-02], device='cuda:0')
    tensor([[ 0.0232,  0.0320, -0.0467,  ..., -0.0146,  0.1355, -0.0660],
            [-0.1252,  0.0975, -0.0480,  ...,  0.0743,  0.0768,  0.0793],
            [-0.0047, -0.0139,  0.0797,  ..., -0.0309, -0.0197,  0.1332],
            ...,
            [-0.0082,  0.0609, -0.0154,  ..., -0.1109,  0.0738, -0.0265],
            [ 0.0768,  0.0514, -0.0610,  ...,  0.1200,  0.0571,  0.1056],
            [-0.0661,  0.0186,  0.0459,  ..., -0.1339, -0.1059,  0.0483]],
           device='cuda:0')
    tensor([[-0.0713,  0.0746,  0.1007,  ...,  0.0840, -0.0825, -0.1065],
            [-0.1172, -0.0272, -0.0440,  ...,  0.0494,  0.0970, -0.0654],
            [-0.0020,  0.0230,  0.0870,  ..., -0.0113, -0.0692,  0.0630],
            ...,
            [ 0.0391,  0.0966,  0.0745,  ...,  0.0983, -0.1164, -0.0110],
            [ 0.0424, -0.1134,  0.0021,  ..., -0.0728,  0.0872, -0.1178],
            [-0.1236, -0.1291, -0.0204,  ..., -0.1377, -0.0294,  0.1127]],
           device='cuda:0')
    tensor([-0.0856, -0.1247, -0.1014,  0.0422,  0.0609,  0.0999,  0.0662, -0.0037,
             0.1104,  0.1023,  0.1271, -0.0234,  0.0726, -0.1007, -0.0202, -0.0048,
            -0.0313,  0.1006, -0.0932,  0.1044, -0.0835,  0.0011,  0.0064,  0.0823,
            -0.0246, -0.0100,  0.0341,  0.0345,  0.0723, -0.0749, -0.0084, -0.0790,
            -0.0336,  0.1200,  0.0436,  0.0458,  0.0620, -0.0090,  0.0633,  0.0659,
             0.0127,  0.0843,  0.0843,  0.0726, -0.0650, -0.0622,  0.0234, -0.0901,
            -0.0160,  0.0494, -0.0337, -0.0163, -0.0406,  0.0948,  0.0201, -0.0439,
            -0.0377, -0.1042,  0.1343,  0.1071,  0.1106, -0.0058, -0.0471,  0.1244,
            -0.0250, -0.0441,  0.0876, -0.0536, -0.0761,  0.1209, -0.0254,  0.0313,
            -0.0781,  0.1061,  0.0508, -0.0198,  0.1219,  0.0624, -0.0635,  0.0053,
            -0.0464, -0.0799,  0.0194, -0.0743, -0.0533, -0.0464,  0.0004, -0.0376,
            -0.0451, -0.0268, -0.0756, -0.0185, -0.0966, -0.0385, -0.0118,  0.1259,
             0.0783, -0.0160, -0.1252, -0.0559,  0.0136, -0.0990,  0.1337, -0.0095,
            -0.1082,  0.0435, -0.0781,  0.0462,  0.0968, -0.0907, -0.1060,  0.0651,
            -0.1162, -0.0170,  0.1141, -0.0522, -0.0513, -0.1139, -0.1122, -0.0972,
            -0.0086,  0.0532,  0.0740, -0.0916, -0.0751,  0.0068,  0.0641,  0.0773,
             0.0366,  0.0570, -0.0593, -0.0428,  0.0470, -0.0797,  0.0454, -0.0892,
             0.0135,  0.0248,  0.0161, -0.0562,  0.0944,  0.0751,  0.0667,  0.0992,
             0.0293, -0.0138,  0.0145, -0.0198,  0.0747, -0.0466,  0.1069,  0.0108,
             0.0491,  0.0405, -0.0090, -0.1040,  0.0829, -0.0022,  0.0863, -0.0440,
             0.0853, -0.0841,  0.0106, -0.0034, -0.0051,  0.0984, -0.0642,  0.0459,
             0.0688, -0.1211,  0.0905,  0.0377,  0.0984, -0.0560, -0.0705,  0.0784,
             0.0312, -0.0534, -0.0404, -0.0014,  0.0250, -0.0066,  0.0993,  0.0514,
             0.0415,  0.0132, -0.0393,  0.1004, -0.0973, -0.0016, -0.0592,  0.0972,
            -0.0586,  0.0893, -0.1044,  0.0005, -0.0318,  0.0008,  0.0701,  0.0711,
             0.0433, -0.0585,  0.0220,  0.0300, -0.0849, -0.1076, -0.0723, -0.0845,
             0.0005,  0.0403, -0.0131,  0.0316, -0.0567, -0.0742,  0.1219, -0.0296,
            -0.0929,  0.0716,  0.0031, -0.0612,  0.0201,  0.0837, -0.0120,  0.0089,
             0.0813,  0.0932,  0.0230,  0.0977, -0.0259,  0.0398,  0.0238,  0.1334,
            -0.0576, -0.0088, -0.1080,  0.0644, -0.0582, -0.1038, -0.0384, -0.0203,
             0.0476,  0.0351, -0.0994,  0.0919, -0.1113, -0.0832, -0.0721,  0.0951,
            -0.0140, -0.0209,  0.1131, -0.0193,  0.0407,  0.0527,  0.1050,  0.0825],
           device='cuda:0')
    tensor([ 0.0889,  0.0619, -0.0556,  0.0738,  0.0006,  0.0706,  0.1148, -0.1110,
            -0.1075, -0.0151, -0.0243, -0.0474,  0.0590,  0.0368,  0.0532,  0.0299,
            -0.1245,  0.1214,  0.0329,  0.0290, -0.0948, -0.1010,  0.0557, -0.0280,
            -0.0756, -0.0912,  0.0124, -0.0765, -0.1059,  0.0179,  0.0897, -0.0929,
             0.0018, -0.0241, -0.0790,  0.0608, -0.0410, -0.0022,  0.0143,  0.0782,
             0.0908,  0.1114, -0.0856, -0.0370, -0.1113,  0.0928,  0.0542, -0.0158,
            -0.0271,  0.0527, -0.1023, -0.0463,  0.0624,  0.0285,  0.0334, -0.0171,
             0.0804, -0.0987,  0.1318,  0.1464, -0.0315, -0.0135,  0.0798, -0.0080,
            -0.0627, -0.0994, -0.0894,  0.1226,  0.0353, -0.0531, -0.0253, -0.0977,
            -0.0963, -0.0064,  0.1069,  0.1101, -0.0017, -0.0613, -0.0280,  0.1057,
            -0.0990,  0.0224, -0.0037, -0.0929,  0.0283, -0.0831, -0.1177,  0.1092,
            -0.0480,  0.1127,  0.1401,  0.0542, -0.0422,  0.0820, -0.0568, -0.0832,
            -0.0498,  0.1334,  0.0087, -0.1237,  0.0214,  0.0288,  0.0991, -0.0256,
            -0.0730,  0.0592, -0.0938,  0.0656,  0.0687, -0.0770, -0.0558,  0.0165,
             0.0160, -0.0580,  0.0836, -0.0219,  0.0625,  0.0263,  0.0668, -0.0689,
            -0.0074, -0.0051, -0.0443,  0.0735,  0.0165,  0.0427, -0.0585, -0.0554,
             0.0235, -0.0913, -0.0118,  0.0938,  0.0880,  0.0583, -0.0488, -0.0349,
            -0.0787,  0.0342,  0.0402, -0.1008,  0.0936,  0.0223, -0.0450,  0.0938,
             0.0014, -0.0124,  0.0752, -0.0715,  0.0153, -0.0812, -0.0368, -0.0781,
            -0.0234,  0.0231, -0.0575,  0.0096, -0.0634, -0.0963, -0.0309,  0.1187,
            -0.0010,  0.0066, -0.0541, -0.0901, -0.0874, -0.0267,  0.0312, -0.0510,
            -0.0319,  0.0186, -0.1003, -0.0696,  0.0183, -0.0439, -0.0019,  0.0653,
            -0.1026,  0.0106,  0.0526, -0.0093,  0.0231,  0.1035,  0.1043,  0.0672,
             0.0704, -0.0941, -0.0136, -0.0370, -0.0008,  0.0611, -0.0890, -0.0953,
             0.0526, -0.1076,  0.0325, -0.0087,  0.0060, -0.0533,  0.0136, -0.0305,
            -0.1148, -0.0786,  0.1336, -0.0076,  0.0365,  0.0810,  0.0055, -0.0303,
            -0.0891, -0.0541, -0.0076, -0.0928,  0.0943,  0.0592,  0.0108,  0.1095,
             0.1183, -0.1159, -0.0613, -0.0962,  0.0798, -0.0297, -0.0229,  0.0605,
             0.0924,  0.0163, -0.1299, -0.0247,  0.0640,  0.1281,  0.0440,  0.0082,
            -0.0785, -0.0689,  0.0901, -0.0126, -0.0945, -0.0886, -0.0066,  0.0728,
             0.0943, -0.0231, -0.0614,  0.1039,  0.1051,  0.0180,  0.0294,  0.0958,
            -0.1084,  0.0418, -0.1075, -0.0455,  0.0083,  0.1402, -0.0365,  0.1264],
           device='cuda:0')
    tensor([[ 0.1042,  0.1307, -0.0339,  ...,  0.0423, -0.0060,  0.1093],
            [ 0.0867,  0.0573, -0.0844,  ...,  0.0621,  0.0789,  0.1218],
            [ 0.0349, -0.0302, -0.0875,  ..., -0.1394,  0.1217, -0.1109],
            ...,
            [ 0.1279, -0.0845,  0.1338,  ...,  0.1450, -0.0627,  0.0487],
            [-0.0129, -0.0288,  0.0744,  ...,  0.0994,  0.1064, -0.0595],
            [-0.0666, -0.0271,  0.0480,  ..., -0.0160, -0.0130, -0.0867]],
           device='cuda:0')
    tensor([[-0.0233, -0.0447, -0.0769,  ...,  0.1124,  0.0431, -0.0644],
            [ 0.0309, -0.1125,  0.0957,  ..., -0.1197,  0.0604, -0.0897],
            [-0.1142,  0.1258, -0.0171,  ...,  0.0529, -0.0998, -0.0107],
            ...,
            [-0.0058,  0.1296,  0.0664,  ..., -0.0093,  0.0843, -0.0714],
            [-0.0014,  0.1041, -0.0164,  ..., -0.0924, -0.0404, -0.0521],
            [ 0.0330,  0.0804, -0.1365,  ...,  0.0555, -0.0812,  0.0388]],
           device='cuda:0')
    tensor([ 0.0696, -0.1020,  0.0913, -0.0353, -0.0380, -0.0187,  0.1108, -0.0785,
            -0.0989,  0.0663,  0.0885,  0.0988, -0.0172, -0.0486,  0.0444,  0.1037,
             0.0388, -0.0568, -0.0007, -0.0404, -0.0754, -0.0292, -0.1205,  0.0246,
            -0.0417,  0.1263,  0.0300,  0.0790, -0.1128,  0.1117, -0.0958,  0.0376,
            -0.0008,  0.0507, -0.0563,  0.0683,  0.0859,  0.0146, -0.1361, -0.1278,
             0.0040, -0.0347,  0.0858,  0.0823,  0.0932, -0.0544, -0.0160,  0.1027,
            -0.0347, -0.1235,  0.0732, -0.0056, -0.0069,  0.0655,  0.1055,  0.0897,
            -0.1013,  0.0268, -0.0939,  0.0751, -0.1188, -0.0588, -0.0095, -0.0868,
            -0.0173,  0.0533, -0.0480,  0.0887, -0.0435,  0.0964, -0.0009, -0.0195,
             0.0356, -0.0110,  0.0742,  0.1072, -0.0541,  0.0832,  0.0734,  0.0965,
            -0.0799, -0.0366,  0.0259, -0.0766, -0.0956, -0.0989, -0.1294, -0.0696,
             0.1086, -0.0826, -0.0721, -0.0317,  0.0649,  0.0178,  0.1263,  0.0364,
             0.0077,  0.1096,  0.0130, -0.0925,  0.0293,  0.0925, -0.0315, -0.0020,
            -0.0730, -0.0805, -0.0914, -0.0774,  0.0553,  0.1015, -0.0424,  0.0605,
            -0.1108,  0.0596,  0.0807,  0.0405,  0.0721,  0.0857, -0.1118, -0.0459,
            -0.0259,  0.1066, -0.0345, -0.0739, -0.0079, -0.0473, -0.1142, -0.0367,
            -0.0428, -0.0301,  0.0264,  0.1210,  0.1035, -0.0428,  0.1057,  0.0934,
            -0.0601,  0.0959, -0.0176, -0.1130, -0.0388, -0.0041, -0.0226, -0.0552,
            -0.0383,  0.1165,  0.0731,  0.0728, -0.0237,  0.0418, -0.1111, -0.0167,
            -0.0427, -0.0045, -0.0615, -0.1189, -0.0353,  0.0194, -0.1171, -0.1190,
            -0.0631, -0.0979,  0.1002,  0.0660, -0.0770, -0.0546,  0.0003, -0.0584,
             0.1081,  0.0059,  0.0717, -0.0260,  0.0364,  0.1009, -0.0013, -0.0039,
             0.0524, -0.0902,  0.0268,  0.0102, -0.0310,  0.0631, -0.0039, -0.0724,
            -0.0278, -0.0938, -0.0237, -0.0090,  0.1170, -0.0298, -0.0204,  0.1028,
            -0.0629,  0.0104, -0.1348,  0.0787,  0.0783, -0.0152, -0.0084,  0.0345,
             0.0666,  0.0643,  0.1112, -0.1283, -0.0049, -0.1330, -0.1055, -0.0319,
            -0.1292, -0.0660, -0.0399, -0.0554, -0.1222, -0.1161,  0.0171, -0.0603,
             0.0309,  0.0859, -0.0644, -0.1120,  0.0442,  0.0994, -0.0084,  0.0066,
             0.0273, -0.0847,  0.0019,  0.1111, -0.0840, -0.1361, -0.0456, -0.0461,
             0.1111,  0.0143, -0.0729, -0.0332, -0.0855, -0.0923, -0.0920,  0.0596,
            -0.1155,  0.0579,  0.0884,  0.0531, -0.0738,  0.0935, -0.0317,  0.0250,
             0.0787,  0.0632,  0.0752,  0.0935, -0.1197,  0.0549,  0.1068,  0.0384],
           device='cuda:0')
    tensor([-6.0003e-02, -3.7581e-02,  5.9247e-02,  9.1638e-02, -6.0104e-02,
             3.6220e-02,  1.2154e-01,  3.6464e-02, -4.2102e-02,  7.2700e-02,
             3.6828e-02,  4.8468e-02, -1.8912e-02,  3.4630e-02,  1.2566e-02,
             7.2549e-02,  5.3780e-02, -9.2877e-02,  7.7762e-02,  5.7719e-03,
             7.9732e-02, -3.8764e-02, -4.6069e-02,  3.5891e-02,  4.4258e-02,
            -2.9272e-02, -6.3248e-02,  1.8195e-02, -5.2263e-02, -8.3334e-02,
            -8.5980e-02, -4.7582e-02, -2.0810e-02, -4.6726e-03,  1.1369e-01,
             1.0170e-01,  9.3305e-02, -5.9080e-02, -6.9313e-02, -7.2970e-02,
            -1.2447e-01, -8.2902e-02, -1.4974e-02,  1.0879e-01, -6.5175e-02,
            -9.0298e-02, -1.0500e-01,  1.3356e-02, -5.3745e-02,  1.1134e-01,
             8.8578e-02, -3.4906e-02, -6.7157e-02,  9.7101e-02, -8.8895e-02,
            -4.6294e-02,  9.6088e-02,  1.1415e-01, -1.6201e-02, -7.3066e-02,
             2.7254e-02,  5.2121e-02, -5.3740e-02, -4.8144e-02, -6.8342e-02,
             8.7422e-04, -6.5132e-02, -9.3278e-02,  8.2424e-03, -5.3566e-02,
             1.2051e-04, -9.1191e-02,  1.0129e-01,  1.9481e-02,  6.6510e-02,
             4.6962e-03,  9.9682e-03, -1.1756e-01, -1.8117e-03, -7.6454e-02,
             5.2457e-02, -1.9523e-02,  1.4146e-03,  8.0378e-02,  9.8027e-02,
             1.2571e-01,  4.6454e-02,  1.1226e-01,  6.1128e-02, -3.8319e-02,
             1.0489e-01, -5.4454e-02,  3.5309e-02,  6.9736e-02, -2.7209e-02,
             4.5208e-03,  8.8196e-02,  7.8905e-02, -1.1429e-01,  6.1355e-02,
             1.2507e-02,  4.0660e-02,  8.1279e-02,  9.1690e-04, -7.5795e-02,
            -5.6711e-02, -5.6377e-02, -7.4999e-02,  5.5531e-02, -7.8009e-02,
             1.0810e-02, -1.0215e-01,  1.1060e-01,  3.5436e-02,  9.1269e-02,
             2.5669e-02,  8.0246e-02,  1.2873e-01, -1.0426e-01,  7.2654e-02,
             6.4723e-02,  8.7931e-02, -8.4807e-02, -1.0219e-02,  7.7049e-02,
            -1.5545e-03, -1.2343e-01, -9.6953e-02,  7.3606e-02, -3.8960e-02,
            -3.5523e-02,  7.8131e-02, -1.2750e-01, -1.0680e-01, -8.6382e-02,
             7.0688e-02,  7.7483e-02, -4.0641e-02,  8.4208e-02, -8.4943e-02,
            -2.9726e-02, -4.2109e-02, -4.8205e-02, -1.0646e-01,  6.2740e-02,
             1.0202e-01,  8.5782e-02,  3.6998e-02,  1.0878e-01,  9.4561e-02,
             4.6990e-02,  1.0403e-01,  2.5804e-02,  1.0120e-01,  1.4204e-02,
             3.8619e-02,  1.7528e-02,  8.9934e-02,  1.2231e-02, -1.4055e-02,
             2.2923e-02,  4.7291e-03, -1.6853e-02, -1.1629e-02, -1.1910e-01,
             4.5359e-02,  4.0438e-02,  3.7086e-02,  8.9290e-02, -1.2077e-01,
             3.9873e-02, -6.1061e-02, -9.7222e-03, -7.8936e-02, -2.1062e-02,
             1.0168e-01, -1.7111e-02, -1.0337e-01,  4.7872e-02,  1.0774e-01,
             1.0063e-01, -4.4212e-02, -5.7033e-02,  4.2120e-02, -1.2005e-01,
             5.9090e-03,  1.5068e-02, -1.1531e-01, -4.4630e-02, -9.5794e-02,
             8.6880e-02, -1.3191e-01, -2.2800e-03,  1.1835e-02, -7.4690e-02,
            -4.0941e-02,  1.2987e-02, -3.4841e-02, -1.0015e-01, -6.1040e-02,
            -7.7662e-02,  6.0571e-02,  2.9924e-02,  8.9255e-02,  9.2347e-02,
            -9.0451e-02, -3.8404e-02, -1.1567e-01,  2.8534e-02,  1.8380e-02,
            -9.0496e-02, -7.1661e-02, -8.1346e-02, -1.1905e-01,  7.8951e-02,
            -1.2349e-02,  5.3665e-02, -5.9657e-02, -9.2168e-02,  9.9249e-03,
            -1.3755e-01, -1.0770e-01, -9.4613e-02,  8.4539e-02,  5.5467e-02,
            -1.1428e-02,  8.5340e-02,  7.4581e-02,  1.1140e-01, -1.2238e-01,
            -6.5725e-02, -1.0739e-01, -7.3792e-02, -8.3347e-02, -7.6483e-02,
            -1.2934e-01,  6.2804e-02,  1.1574e-01,  8.7894e-02, -1.1685e-01,
             1.5607e-02, -4.0669e-02, -9.7910e-02, -3.3280e-02, -4.3729e-02,
             6.5864e-02,  1.0552e-01, -7.6241e-02,  7.3893e-02,  7.0290e-03,
            -1.1837e-01, -4.7529e-02,  6.5243e-02, -9.9351e-02, -1.8611e-02,
            -3.9569e-02], device='cuda:0')
    tensor([[-0.1136, -0.0049, -0.0863,  0.0839, -0.0431,  0.0720, -0.0754,  0.0587,
              0.0388, -0.0890,  0.0887,  0.0521,  0.0298, -0.0561, -0.0076,  0.0679,
             -0.0869, -0.1098,  0.1289,  0.1051, -0.0713,  0.0983, -0.0061, -0.0013,
              0.0399,  0.1140, -0.0728,  0.1209, -0.0054,  0.0936, -0.1109,  0.1325,
             -0.1052, -0.0177,  0.0295,  0.1134, -0.1144,  0.0241,  0.0193, -0.1014,
             -0.0460, -0.0415,  0.0890, -0.0245, -0.0257,  0.0975,  0.0064,  0.0185,
             -0.0383,  0.0981, -0.0201,  0.0518,  0.0217, -0.1181,  0.0744, -0.0941,
              0.0706, -0.0512, -0.0367, -0.1284, -0.0883,  0.0660, -0.0860, -0.0753]],
           device='cuda:0')
    tensor([0.0988], device='cuda:0')


## Part 5: Checking model performance

Make prediction on training and testing dataset


```python
plt.plot(history)
# dplt.plot(history.history['val_loss'])
```




    [<matplotlib.lines.Line2D at 0x7f8d7c51e978>]




![png](/graph/output_19_1.png)



```python
X_test[0]
```




    array([[0.75733001, 0.74433354, 0.74433354, 0.73778332, 0.74246205,
            0.74069453, 0.74474943]])




```python
# X_train_X_test = np.concatenate((X_train, X_test),axis=0)
# hidden_state = None
rnn.eval()
# test_inputs = torch.tensor(X_test).float().cuda()
test_predict, _ = rnn(X_test_cuda, hidden_state)
test_predict_cpu = test_predict.cpu().detach().numpy()
```


```python
test_predict_cpu.shape
```




    (251, 1, 1)




```python
plt.plot(sc.inverse_transform(y_test.reshape(-1,1)))
plt.plot(sc.inverse_transform(test_predict_cpu.reshape(-1,1)))
```




    [<matplotlib.lines.Line2D at 0x7f8d7c4d3c50>]




![png](/graph/output_23_1.png)



```python
# plot original data
plt.plot(sc.inverse_transform(y.reshape(-1,1)), color='k')

# train_inputs = torch.tensor(X_train).float().cuda()
train_pred, hidden_state = rnn(inputs_cuda, None)
train_pred_cpu = train_pred.cpu().detach().numpy()

# use hidden state from previous training data
test_predict, _ = rnn(X_test_cuda, hidden_state)
test_predict_cpu = test_predict.cpu().detach().numpy()

# plt.plot(scl.inverse_transform(y_test.reshape(-1,1)))
split_pt = int(X.shape[0] * 0.80) + 7 # window_size
plt.plot(np.arange(7, split_pt, 1), sc.inverse_transform(train_pred_cpu.reshape(-1,1)), color='b')
plt.plot(np.arange(split_pt, split_pt + len(test_predict_cpu), 1), sc.inverse_transform(test_predict_cpu.reshape(-1,1)), color='r')

# pretty up graph
plt.xlabel('day')
plt.ylabel('price of MMM stock')
plt.legend(['original series','training fit','testing fit'], loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()

```


![png](/graph/output_24_0.png)



```python

```
